{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Analyse & Qualité des Données - Dataset Cancer du Sein\n",
    "# - Fusion données tabulaires & images\n",
    "# - Exploration & visualisation\n",
    "# - Analyse de la qualité des données\n",
    "# - Préparation d'un dataset nettoyé\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optionnel : seaborn si tu veux plus de style\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "\n",
    "BASE = Path(\"CancerSeins\")   # adapter si besoin\n",
    "CSV_DIR = BASE / \"csv\"\n",
    "IMG_DIR = BASE / \"jpeg\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 180)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 1. Chargement & fusion des données\n",
    "\n",
    "# %%\n",
    "# Charger les fichiers CSV\n",
    "meta = pd.read_csv(CSV_DIR / \"meta.csv\")\n",
    "dicom_info = pd.read_csv(CSV_DIR / \"dicom_info.csv\")\n",
    "\n",
    "calc_train = pd.read_csv(CSV_DIR / \"calc_case_description_train_set.csv\")\n",
    "calc_test  = pd.read_csv(CSV_DIR / \"calc_case_description_test_set.csv\")\n",
    "mass_train = pd.read_csv(CSV_DIR / \"mass_case_description_train_set.csv\")\n",
    "mass_test  = pd.read_csv(CSV_DIR / \"mass_case_description_test_set.csv\")\n",
    "\n",
    "# Concat train + test pour chaque type de lésion\n",
    "calc = pd.concat([calc_train, calc_test], ignore_index=True)\n",
    "mass = pd.concat([mass_train, mass_test], ignore_index=True)\n",
    "\n",
    "print(\"meta:\", meta.shape)\n",
    "print(\"dicom_info:\", dicom_info.shape)\n",
    "print(\"calc:\", calc.shape)\n",
    "print(\"mass:\", mass.shape)\n",
    "\n",
    "# %%\n",
    "# Fusion meta + dicom_info\n",
    "df = meta.merge(dicom_info, on=\"image_id\", how=\"left\")\n",
    "\n",
    "# Ajouter description des calcifications & masses\n",
    "df = df.merge(calc, on=\"image_id\", how=\"left\", suffixes=(\"\", \"_calc\"))\n",
    "df = df.merge(mass, on=\"image_id\", how=\"left\", suffixes=(\"\", \"_mass\"))\n",
    "\n",
    "print(\"DataFrame fusionné:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. Ajout du lien vers les images JPEG\n",
    "\n",
    "# %%\n",
    "def find_first_jpg(dicom_id):\n",
    "    \"\"\"Retourne le chemin du premier .jpg présent dans le dossier dicom_id.\"\"\"\n",
    "    folder_path = IMG_DIR / str(dicom_id)\n",
    "    if folder_path.exists() and folder_path.is_dir():\n",
    "        jpgs = list(folder_path.glob(\"*.jpg\"))\n",
    "        if jpgs:\n",
    "            return jpgs[0].as_posix()\n",
    "    return np.nan\n",
    "\n",
    "# attention : peut prendre un peu de temps la première fois\n",
    "df[\"image_path\"] = df[\"dicom_id\"].apply(find_first_jpg)\n",
    "\n",
    "# Statistiques de correspondance\n",
    "n_total = len(df)\n",
    "n_found = df[\"image_path\"].notna().sum()\n",
    "print(f\"Images trouvées : {n_found}/{n_total} ({n_found/n_total*100:.2f} %)\")\n",
    "\n",
    "df[[\"image_id\", \"dicom_id\", \"image_path\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 3. Vue d'ensemble des données\n",
    "\n",
    "# %%\n",
    "print(\"Dimensions :\", df.shape)\n",
    "\n",
    "print(\"\\nTypes des colonnes :\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# %%\n",
    "# Aperçu aléatoire\n",
    "df.sample(5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Qualité des données : valeurs manquantes & doublons\n",
    "\n",
    "# %%\n",
    "# Fonction utilitaire pour résumé des NA\n",
    "def missing_report(dataframe):\n",
    "    na_count = dataframe.isna().sum()\n",
    "    na_pct = na_count / len(dataframe) * 100\n",
    "    rep = pd.DataFrame({\n",
    "        \"nb_manquants\": na_count,\n",
    "        \"%_manquants\": na_pct\n",
    "    })\n",
    "    rep = rep[rep[\"nb_manquants\"] > 0].sort_values(\"%_manquants\", ascending=False)\n",
    "    return rep\n",
    "\n",
    "missing_df = missing_report(df)\n",
    "missing_df.head(20)\n",
    "\n",
    "# %%\n",
    "# Visualisation des % de manquants (top 30)\n",
    "if not missing_df.empty:\n",
    "    top_missing = missing_df.head(30)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(top_missing.index, top_missing[\"%_manquants\"])\n",
    "    plt.xlabel(\"% de valeurs manquantes\")\n",
    "    plt.title(\"Top 30 des variables avec valeurs manquantes\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "# Doublons sur image_id (clé logique)\n",
    "n_duplicates = df.duplicated(subset=[\"image_id\"]).sum()\n",
    "print(f\"Doublons potentiels sur image_id : {n_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Statistiques descriptives & cardinalité\n",
    "\n",
    "# %%\n",
    "# Colonnes numériques (hors identifiants)\n",
    "id_like = [\"image_id\", \"patient_id\", \"study_id\", \"dicom_id\"]\n",
    "num_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c not in id_like]\n",
    "\n",
    "df[num_cols].describe().T\n",
    "\n",
    "# %%\n",
    "# Cardinalité des variables catégorielles\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "card = pd.DataFrame({\n",
    "    \"nb_modalites\": [df[c].nunique(dropna=True) for c in cat_cols],\n",
    "    \"nb_manquants\": [df[c].isna().sum() for c in cat_cols]\n",
    "}, index=cat_cols).sort_values(\"nb_modalites\", ascending=False)\n",
    "\n",
    "card.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 6. Distribution de la variable cible\n",
    "\n",
    "# %%\n",
    "# Essaie plusieurs noms possibles selon tes colonnes\n",
    "target_candidates = [c for c in df.columns if \"path\" in c.lower() or \"malign\" in c.lower() or \"class\" in c.lower()]\n",
    "target_candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 8. Visualisations bivariées\n",
    "\n",
    "# %%\n",
    "if TARGET_COL in df.columns:\n",
    "    # Boxplots pour quelques variables numériques\n",
    "    for col in num_cols[:8]:  # limite à 8 pour ne pas exploser\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.boxplot(x=df[TARGET_COL], y=df[col])\n",
    "        plt.title(f\"{col} selon {TARGET_COL}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 7. Visualisations univariées (variables numériques)\n",
    "\n",
    "# %%\n",
    "for col in num_cols:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(df[col].dropna(), bins=30)\n",
    "    plt.title(f\"Distribution de {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Effectif\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 9. Matrice de corrélation\n",
    "\n",
    "# %%\n",
    "corr = df[num_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Matrice de corrélation (variables numériques)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(\"../..\") / \"CancerSeins\"\n",
    "\n",
    "# --- 1. Charger les CSV ---\n",
    "meta = pd.read_csv(BASE / \"csv/meta.csv\")\n",
    "dicom_info = pd.read_csv(BASE / \"csv/dicom_info.csv\")\n",
    "calc_train = pd.read_csv(BASE / \"csv/calc_case_description_train_set.csv\")\n",
    "calc_test = pd.read_csv(BASE / \"csv/calc_case_description_test_set.csv\")\n",
    "mass_train = pd.read_csv(BASE / \"csv/mass_case_description_train_set.csv\")\n",
    "mass_test = pd.read_csv(BASE / \"csv/mass_case_description_test_set.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta DataFrame:\n",
      "                                   SeriesInstanceUID  \\\n",
      "0  1.3.6.1.4.1.9590.100.1.2.117041576511324414842...   \n",
      "1  1.3.6.1.4.1.9590.100.1.2.438738396107617880132...   \n",
      "2  1.3.6.1.4.1.9590.100.1.2.767416741131676463382...   \n",
      "3  1.3.6.1.4.1.9590.100.1.2.296931352612305599800...   \n",
      "4  1.3.6.1.4.1.9590.100.1.2.436657670120353100077...   \n",
      "\n",
      "                                    StudyInstanceUID Modality  \\\n",
      "0  1.3.6.1.4.1.9590.100.1.2.229361142710768138411...       MG   \n",
      "1  1.3.6.1.4.1.9590.100.1.2.195593486612988388325...       MG   \n",
      "2  1.3.6.1.4.1.9590.100.1.2.257901172612530623323...       MG   \n",
      "3  1.3.6.1.4.1.9590.100.1.2.109468616710242115222...       MG   \n",
      "4  1.3.6.1.4.1.9590.100.1.2.380627129513562450304...       MG   \n",
      "\n",
      "  SeriesDescription BodyPartExamined  SeriesNumber Collection  Visibility  \\\n",
      "0   ROI mask images           BREAST             1  CBIS-DDSM           1   \n",
      "1   ROI mask images           BREAST             1  CBIS-DDSM           1   \n",
      "2   ROI mask images           BREAST             1  CBIS-DDSM           1   \n",
      "3   ROI mask images           BREAST             1  CBIS-DDSM           1   \n",
      "4   ROI mask images           BREAST             1  CBIS-DDSM           1   \n",
      "\n",
      "   ImageCount  \n",
      "0           2  \n",
      "1           2  \n",
      "2           2  \n",
      "3           2  \n",
      "4           2  \n"
     ]
    }
   ],
   "source": [
    "print(\"Meta DataFrame:\")\n",
    "print(meta.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'image_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_2812\\3348353180.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m calc = pd.concat([calc_train, calc_test], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m mass = pd.concat([mass_train, mass_test], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# --- 2. Fusionner meta + dicom ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = meta.merge(dicom_info, on=\u001b[33m\"image_id\"\u001b[39m, how=\u001b[33m\"left\"\u001b[39m)\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# --- 3. Ajouter description des cas ---\u001b[39;00m\n\u001b[32m      9\u001b[39m df = df.merge(calc, on=\"image_id\", how=\"left\") \\\n",
      "\u001b[32mc:\\Users\\angej\\miniconda3\\envs\\VisualisationMassive\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10855\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10856\u001b[39m     ) -> DataFrame:\n\u001b[32m  10857\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10858\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10859\u001b[39m         return merge(\n\u001b[32m  10860\u001b[39m             self,\n\u001b[32m  10861\u001b[39m             right,\n\u001b[32m  10862\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\angej\\miniconda3\\envs\\VisualisationMassive\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\angej\\miniconda3\\envs\\VisualisationMassive\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32mc:\\Users\\angej\\miniconda3\\envs\\VisualisationMassive\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1294\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1295\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1296\u001b[39m                         rk = cast(Hashable, rk)\n\u001b[32m   1297\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m                             right_keys.append(right._get_label_or_level_values(rk))\n\u001b[32m   1299\u001b[39m                         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1300\u001b[39m                             \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[32m   1301\u001b[39m                             right_keys.append(right.index._values)\n",
      "\u001b[32mc:\\Users\\angej\\miniconda3\\envs\\VisualisationMassive\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1910\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1911\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1912\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1913\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1915\u001b[39m \n\u001b[32m   1916\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1917\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'image_id'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fusionner calc + mass\n",
    "calc = pd.concat([calc_train, calc_test], ignore_index=True)\n",
    "mass = pd.concat([mass_train, mass_test], ignore_index=True)\n",
    "\n",
    "# --- 2. Fusionner meta + dicom ---\n",
    "df = meta.merge(dicom_info, on=\"image_id\", how=\"left\")\n",
    "\n",
    "# --- 3. Ajouter description des cas ---\n",
    "df = df.merge(calc, on=\"image_id\", how=\"left\") \\\n",
    "       .merge(mass, on=\"image_id\", how=\"left\")\n",
    "\n",
    "# --- 4. Construire le chemin vers les images ---\n",
    "def get_image_path(row):\n",
    "    folder = row[\"dicom_id\"]\n",
    "    folder_path = BASE / \"jpeg\" / folder\n",
    "    if folder_path.exists():\n",
    "        files = list(folder_path.glob(\"*.jpg\"))\n",
    "        if len(files) > 0:\n",
    "            return str(files[0])\n",
    "    return None\n",
    "\n",
    "df[\"image_path\"] = df.apply(get_image_path, axis=1)\n",
    "\n",
    "print(df.head())\n",
    "print(df[\"image_path\"].notnull().mean()*100, \"% images retrouvées\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VisualisationMassive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
